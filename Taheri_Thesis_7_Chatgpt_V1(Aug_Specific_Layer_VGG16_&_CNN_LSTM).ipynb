{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZaimaSartajTaheri/Blood_Donate/blob/master/Taheri_Thesis_7_Chatgpt_V1(Aug_Specific_Layer_VGG16_%26_CNN_LSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR6ToPn4N7Cm",
        "outputId": "b51664ea-5e8f-4c73-fe9c-66e85e1e8faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9M0yzpZwOLGb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D,GlobalMaxPooling2D, concatenate, Dropout,Conv2D, MaxPooling2D, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7Uk0GZhGN8zJ"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.read_csv('/content/drive/MyDrive/TaheriThesis/Dataset/trainA.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CcMk_grpOYFi"
      },
      "outputs": [],
      "source": [
        "def load_image(path):\n",
        "    img = load_img(path, target_size=(224,224))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import imgaug.augmenters as iaa\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def preprocess_images(data):\n",
        "    images = []\n",
        "\n",
        "    augmenter = iaa.Sequential([\n",
        "        iaa.Fliplr(0.5),  # flip horizontally with 50% probability\n",
        "        iaa.Affine(rotate=(-15, 15)),  # rotate the image by -15 to 15 degrees\n",
        "        iaa.GaussianBlur(sigma=(0, 3.0)),  # apply Gaussian blur with sigma between 0 and 3.0\n",
        "        iaa.Multiply((0.8, 1.2))  # change brightness of the image randomly\n",
        "    ])\n",
        "    for index, row in data.iterrows():\n",
        "        # Image preprocessing and augmentation\n",
        "        path = os.path.join('/content/drive/MyDrive/TaheriThesis/Dataset/Images/', row['image_name'])\n",
        "        img = load_image(path)\n",
        "        img_aug = augmenter.augment_image(img)  # apply image augmentation to the image\n",
        "        images.append(img_aug)\n",
        "        \n",
        "    \n",
        "    \n",
        "    images = np.array(images)\n",
        "  \n",
        "    return images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18dPpbAMrIi-",
        "outputId": "7d843445-9b9b-431a-a5af-9d2b796ef075"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_texts(data):\n",
        "    \n",
        "    texts = []\n",
        "    \n",
        "    for index, row in data.iterrows():\n",
        "       \n",
        "        \n",
        "        # Text preprocessing and augmentation\n",
        "        text = row['Captions']\n",
        "        words = text.split()\n",
        "        if len(words) > 1:\n",
        "            # Synonym replacement\n",
        "            new_words = []\n",
        "            for word in words:\n",
        "                synsets = wordnet.synsets(word)\n",
        "                if synsets:\n",
        "                    synset = synsets[0]\n",
        "                    synonyms = synset.lemma_names()\n",
        "                    if len(synonyms) > 1:\n",
        "                        new_word = synonyms[1]\n",
        "                        new_words.append(new_word)\n",
        "                    else:\n",
        "                        new_words.append(word)\n",
        "                else:\n",
        "                    new_words.append(word)\n",
        "            new_text = \" \".join(new_words)\n",
        "            \n",
        "            # Random insertion/deletion\n",
        "            if np.random.rand() < 0.5:\n",
        "                # Insert a random word\n",
        "                new_words.insert(np.random.randint(len(new_words)), \"RANDOM_WORD\")\n",
        "            else:\n",
        "                # Delete a random word\n",
        "                del new_words[np.random.randint(len(new_words))]\n",
        "            new_text = \" \".join(new_words)\n",
        "            \n",
        "            # Character level augmentation\n",
        "            new_text = new_text.lower()\n",
        "            new_text = new_text.replace(\".\", \"\")\n",
        "            new_text = new_text.replace(\",\", \"\")\n",
        "            new_text = new_text.replace(\"?\", \"\")\n",
        "            new_text = new_text.replace(\"!\", \"\")\n",
        "            new_text = new_text.replace(\"-\", \"\")\n",
        "            new_text = new_text.replace(\"'\", \"\")\n",
        "            new_text = new_text.replace('\"', '')\n",
        "            new_text = list(new_text)\n",
        "            for i in range(len(new_text)):\n",
        "                if np.random.rand() < 0.1:\n",
        "                    new_text[i] = new_text[i].upper()\n",
        "            new_text = \"\".join(new_text)\n",
        "            texts.append(new_text)\n",
        "        else:\n",
        "            texts.append(text)\n",
        "    \n",
        "   \n",
        "    texts = np.array(texts)\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "vCUL9YCsr66K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Tk0z0l3qOhXy"
      },
      "outputs": [],
      "source": [
        "# def preprocess_images(data):\n",
        "#     images = []\n",
        "#     for path in data['image_name']:\n",
        "#         path = os.path.join('/content/drive/MyDrive/TaheriThesis/Dataset/Images/', path)\n",
        "#         img = load_image(path)\n",
        "#         images.append(img)\n",
        "#     images = np.array(images)\n",
        "#     return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPdEqKNyWRqZ",
        "outputId": "fd934738-15eb-434e-e31e-ae137674cf06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3297    6\n",
              "176     4\n",
              "2994    6\n",
              "774     0\n",
              "1578    1\n",
              "       ..\n",
              "2009    4\n",
              "1180    0\n",
              "3441    6\n",
              "1344    1\n",
              "1289    1\n",
              "Name: Label, Length: 4143, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "train_data = shuffle(train_dataset, random_state = 10)\n",
        "labels = train_data['Label']\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zaLaP9Y6DtK3"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, concatenate\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, concatenate, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# define the hyperparameters\n",
        "max_sequence_length = 50 # max sequence length for text\n",
        "embedding_dim = 100 # embedding dimension for text\n",
        "max_words = 20000 # max number of words in the tokenizer\n",
        "num_classes = 7 # number of emotion classes\n",
        "image_size = (224, 224) # size of input image for the VGG16 model\n",
        "\n",
        "# load the VGG16 model with pre-trained weights and without the last layer\n",
        "image_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "# unfreeze the last convolutional block of the VGG16 model\n",
        "for layer in image_model.layers[:-5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# define the text input\n",
        "text_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "\n",
        "# create the text embedding layer\n",
        "text_embedding = Embedding(max_words, embedding_dim, input_length=max_sequence_length)(text_input)\n",
        "\n",
        "# # apply a 1D convolutional layer and max pooling layer to the text embedding\n",
        "# text_conv = Conv1D(128, 5, activation='relu')(text_embedding)\n",
        "# text_pool = MaxPooling1D(5)(text_conv)\n",
        "# text_drop = Dropout(0.2)(text_pool)\n",
        "# text_lstm = LSTM(100)(text_drop)\n",
        "\n",
        "# # get the output of the image model\n",
        "# image_output = image_model.output\n",
        "# image_flat = Flatten()(image_output)\n",
        "# image_dense = Dense(128, activation='relu')(image_flat)\n",
        "# image_drop = Dropout(0.2)(image_dense)\n",
        "\n",
        "# # concatenate the text and image features\n",
        "# merged = concatenate([image_drop, text_lstm])\n",
        "\n",
        "# # apply a final fully connected layer to get the output probabilities\n",
        "# output = Dense(num_classes, activation='softmax')(merged)\n",
        "\n",
        "# # create the final model\n",
        "# model = Model(inputs=[image_model.input, text_input], outputs=output)\n",
        "\n",
        "# # compile the model\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # load the data\n",
        "# train_data = shuffle(train_dataset, random_state = 10)\n",
        "# images = preprocess_images(train_data)\n",
        "# texts = train_data['Captions']\n",
        "# labels = train_data['Label']\n",
        "# labels = to_categorical(labels)\n",
        "\n",
        "# # split the data into training and testing sets\n",
        "# train_images, test_images, train_texts, test_texts, train_labels, test_labels = train_test_split(images, texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# # tokenize the text data\n",
        "# tokenizer = Tokenizer(num_words=max_words)\n",
        "# tokenizer.fit_on_texts(train_texts)\n",
        "# train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "# test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "# train_text_data = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "# test_text_data = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# # train the model\n",
        "# model.fit([train_images, train_text_data], train_labels, batch_size=64, epochs=10, validation_data=([test_images, test_text_data], test_labels))\n",
        "\n",
        "# score, acc = model.evaluate([test_images, test_text_data], test_labels, batch_size=64)\n",
        "# print('Test score:', score)\n",
        "# print('Test accuracy:', acc)\n",
        "\n",
        "# # from tensorflow.keras.models import load_model\n",
        "# # loaded_model = load_model('emotion_detection_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yMVj4SzRUDGp"
      },
      "outputs": [],
      "source": [
        "# apply a 1D convolutional layer and max pooling layer to the text embedding\n",
        "text_conv = Conv1D(128, 5, activation='relu')(text_embedding)\n",
        "text_pool = MaxPooling1D(5)(text_conv)\n",
        "text_drop = Dropout(0.2)(text_pool)\n",
        "text_lstm = LSTM(100)(text_drop)\n",
        "\n",
        "# get the output of the image model\n",
        "image_output = image_model.output\n",
        "image_flat = Flatten()(image_output)\n",
        "image_dense = Dense(128, activation='relu')(image_flat)\n",
        "image_drop = Dropout(0.2)(image_dense)\n",
        "\n",
        "# concatenate the text and image features\n",
        "merged = concatenate([image_drop, text_lstm])\n",
        "\n",
        "# apply a final fully connected layer to get the output probabilities\n",
        "output = Dense(num_classes, activation='softmax')(merged)\n",
        "\n",
        "# create the final model\n",
        "model = Model(inputs=[image_model.input, text_input], outputs=output)\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = shuffle(train_dataset, random_state = 10)\n",
        "images = preprocess_images(train_data)"
      ],
      "metadata": {
        "id": "Sjpv8cfw6FD2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "buyVWK_AUWMU"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "\n",
        "texts =  preprocess_texts(train_data)\n",
        "labels = train_data['Label']\n",
        "labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RY9qIAr4UW9d"
      },
      "outputs": [],
      "source": [
        "# split the data into training and testing sets\n",
        "train_images, test_images, train_texts, test_texts, train_labels, test_labels = train_test_split(images, texts, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "train_text_data = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "test_text_data = pad_sequences(test_sequences, maxlen=max_sequence_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxCiEuGBUfWp"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "# model.fit([train_images, train_text_data], train_labels, batch_size=32, epochs=10, validation_data=([test_images, test_text_data], test_labels))\n",
        "model.fit([train_images, train_text_data], train_labels, batch_size=32, epochs=8, validation_split =0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IZqcn71CQIZ7"
      },
      "outputs": [],
      "source": [
        "y_test= np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LbUF_gNYUfcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282756ec-c7cc-4372-87a6-c1af66ba4425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 6s 483ms/step - loss: 3.2697 - accuracy: 0.3952\n",
            "Test score: 3.2696714401245117\n",
            "Test accuracy: 0.39518073201179504\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate([test_images, test_text_data], test_labels, batch_size=32)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "# loaded_model = load_model('emotion_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KMr7Wc0-QjYg"
      },
      "outputs": [],
      "source": [
        "# type(y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dMvwDnFJPMUB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "def print_metrices(true,pred):\n",
        "    print(confusion_matrix(true,pred))\n",
        "    print(classification_report(true,pred,target_names=['happy','angry','disgust','fear','sad','surprise','other']))\n",
        "    print(\"Accuracy : \",accuracy_score(true,pred))\n",
        "    print(\"Precison : \",precision_score(true,pred, average = 'weighted'))\n",
        "    print(\"Recall : \",recall_score(true,pred,  average = 'weighted'))\n",
        "    print(\"F1 : \",f1_score(true,pred,  average = 'weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "R0eYmBC0PAHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5620dd4d-fb7c-4d49-8181-f3ad07534441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 2s 116ms/step\n",
            "[[36  2  1  9 10  5 14]\n",
            " [ 3 19  1  5 11  5 11]\n",
            " [ 3  4 14  3  4  3  6]\n",
            " [ 3  2  1 32  3  4  4]\n",
            " [10 11  3  4 24  7 12]\n",
            " [15  2  3  5  4 13  9]\n",
            " [13 12  1  9  7  7 26]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       happy       0.43      0.47      0.45        77\n",
            "       angry       0.37      0.35      0.36        55\n",
            "     disgust       0.58      0.38      0.46        37\n",
            "        fear       0.48      0.65      0.55        49\n",
            "         sad       0.38      0.34      0.36        71\n",
            "    surprise       0.30      0.25      0.27        51\n",
            "       other       0.32      0.35      0.33        75\n",
            "\n",
            "    accuracy                           0.40       415\n",
            "   macro avg       0.41      0.40      0.40       415\n",
            "weighted avg       0.40      0.40      0.39       415\n",
            "\n",
            "Accuracy :  0.39518072289156625\n",
            "Precison :  0.3960875934608163\n",
            "Recall :  0.39518072289156625\n",
            "F1 :  0.3914031192417335\n"
          ]
        }
      ],
      "source": [
        "pred_test = model.predict([test_images, test_text_data])\n",
        "y_pred_test = np.argmax(pred_test, axis=1)\n",
        "y_test_single = np.argmax(y_test, axis=1)\n",
        "\n",
        "print_metrices(y_test_single, y_pred_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}